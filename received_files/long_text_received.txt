Hello everyone. My name is Yan Wang. I interest in utilizing reinforcement learning algorithms on real robots. My project is to increase the efficiency of a very famous reinforcement learning algorithm called PPO. 
slide 2 (talk about the background of deep policy gradient methods and why they are important): 
Recently, reinforcement learning has gained lots of attention in many areas. Typical examples include autonomous driving, robotic control, and video games. Benefiting from the increased computational power, researchers have successfully developped many new algorithms that combine the idea of reinforcement learning and deep neural networks. They are collectively known as Deep Policy Gradient Methods. Most deep policy gradient methods are variants of the most fundamental algorithm called REINFORCE. In some papers it is also called Vanilla Policy Gradient (VPG). My project will focus on one of the most important deep policy gradient methods called Proximal Policy Optimization or PPO for short. PPO has wide applications. You can find its core idea in solving complex video games such as DOTA, Glory of Kings, and StarCraft.

slide 3 (talk about the background of PPO, which is my focus):
In many problems, the vanilla policy gradient method does not have a good performance. That is because it tries to constrain the update in the parameter space. Since not all parameters are equally influential to the policy, the vanilla policy gradient method usually converges to sub-optimal solutions. To fix this problem, people invented Trust Region Policy Optimization, or TRPO for short. Instead of limiting the update size in the parameter space, it tries to limit the difference between the new and the old policies. However, it requires the computation of inverse matrices, which is very expensive using conventional computers. PPO can be considered as a simplified version of TRPO. It uses a simple clip operation to constrain the policy differences. The clipping range is controlled by the episilon parameter which is often 0.1 or 0.2, as shown in the example. The original PPO paper reported similar performance compared to TRPO.

slide 4 (talk about the main drawback of PPO implementations):
Most deep policy gradient methods, including PPO are implemented with PyTorch or TensorFlow. However, the clip function provided by them will remove samples that are unlikely under the current policy, which will significantly reduce the sample efficiency. For complex problems, good actions can be unlikely under the current policy at the early training phase. Since PPO relies on the clip operation to achieve good performance, the optimizer will exclude those good actions from policy improvements. The example here demostrates this problem. The values are calculated using PyTorch. As you can see, the gradients of good actions are 0 after the clip operation. As a result, it will be harder for PPO to find the optimal policy, if not impossible. In practice, reduced sample efficiency means longer training time and dramatically increased cost to adopt reinforcement learning.

slide 5 (talk about my proposed method to mitigate the main drawback of PPO):
Our solution to this problem is replacing the original clip function with our variant. We will first derive a new mathematically equivalent clip function that keeps the gradients of the clipped samples. As the example shows, our clip function's result is the same as that of the original clip function. The difference is that our clip function keeps the gradients of the clipped samples. However, since PPO is an on-policy algorithm and the clipped samples are considered off-policy, our clip function may result in severe overfitting. The example also shows the increased gradients due to the nature of our algorithm. To mitigate this problem, we decide to apply a weighting coefficient to each sample according to their likelihood under the current policy. We will to evaluate the following three weighting schemes: No weighting, Linear, and Exponential.

slide 6 (talk about the evaluation methods and test environments):
We will compare our PPO variant's performance with the original PPO in two dimensions: Cumulative rewards and Training time. We expect our variant to collect no less cumulative rewards within a shorter time frame or collect more cumulative rewards within the same time frame. To account for PPO's wide application range, we will run experiments in various standard environments, including the basic OpenAI environments, Atari games, and Mujoco tasks, to get representative results. To evaluate their performance in actual tasks, we will deploy them on real robots as well. Specifically, Roomba and Anki vector will be used. The same metrics, i.e., cumulative rewards and training time, will be measured for evaluation.

slide 7 (talk about the significance and importance of my project):
Although PPO has been widely adopted in many applications, it is known to suffer from low sample efficiency. In practice, it may take millions of steps for PPO to find a good policy. Collecting so many samples is a challenging task for many applications. Improving sample efficiency will dramatically reduce the number of required samples to find a good policy. A good sample efficiency will also reduce the training time. Due to the trial-and-error nature of reinforcement learning, the training phase may be very costly. For a long time, the lengthy training phase is a significant barrier for industries to adopt reinforcement learning algorithms. A shorter training time means a tremendous reduction in the cost, which will expand the scope of reinforcement learning to many new areas that are impossible before. Thus, a good sample efficiency is essential to the promotion of reinforcement learning. In conclusion, our research will benefit the reinforcement learning community by expanding its application scope and human societies by providing a powerful tool to increase productivity.